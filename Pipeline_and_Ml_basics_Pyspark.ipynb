{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Spark-RDD-2021_solutions",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 2832703719774903
    },
    "colab": {
      "name": "Pipeline and Ml basics Pyspark",
      "provenance": [],
      "collapsed_sections": [
        "levNDESpS1hT",
        "R5ZJa3iU_2bh"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdgarTriquet/PySpark-Projects/blob/main/Pipeline_and_Ml_basics_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzEVmTXWQq-n"
      },
      "source": [
        "# Decision Tree in Spark ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b15737ae-19bd-4b9f-b9f3-52388cc5e50e"
        },
        "id": "levNDESpS1hT"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## Préparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b6afe82e-35d6-4358-9b79-8f56074e0908"
        },
        "id": "4pZBsez9S1hT"
      },
      "source": [
        "Vérifier que des ressources de calcul sont allouées à votre notebook est connecté (cf RAM  de disque indiqués en haut à droite) . Sinon cliquer sur le bouton connecter pour obtenir des ressources.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEjerqQ6Ink6"
      },
      "source": [
        "Pour accéder directement aux fichiers stockées sur votre google drive. Renseigner le code d'authentification lorsqu'il est demandé\n",
        "\n",
        "Ajuster le nom de votre dossier : MyDrive/ens/bdle/dir. **Remplacer dir **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Dv93rsImuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a3a3986-5968-4034-f44c-d8be51e87116"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "drive_dir = \"/content/drive/MyDrive/ens/bdle/dir\"\n",
        "os.makedirs(drive_dir, exist_ok=True)\n",
        "os.listdir(drive_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v9EXMl8aPZC"
      },
      "source": [
        "Installer pyspark et findspark :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zlwNHy1S8C2"
      },
      "source": [
        "!pip install -q pyspark\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnEmOd_zOUo0"
      },
      "source": [
        "Démarrer la session spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0ADH0J-VW7i"
      },
      "source": [
        "import os\n",
        "# !find /usr/local -name \"pyspark\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.7/dist-packages/pyspark\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_WxQZB7TaUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac09c8e-f3b8-4b1e-a885-d3a26f88c1ee"
      },
      "source": [
        "# Principaux import\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession \n",
        "from pyspark import SparkConf  \n",
        "\n",
        "# pour les dataframe et udf\n",
        "from pyspark.sql import *  \n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from datetime import *\n",
        "\n",
        "# pour le chronomètre\n",
        "import time\n",
        "\n",
        "# initialise les variables d'environnement pour spark\n",
        "findspark.init()\n",
        "\n",
        "# Démarrage session spark \n",
        "# --------------------------\n",
        "def demarrer_spark():\n",
        "  local = \"local[*]\"\n",
        "  appName = \"TP\"\n",
        "  configLocale = SparkConf().setAppName(appName).setMaster(local).\\\n",
        "  set(\"spark.executor.memory\", \"6G\").\\\n",
        "  set(\"spark.driver.memory\",\"6G\").\\\n",
        "  set(\"spark.sql.catalogImplementation\",\"in-memory\")\n",
        "  \n",
        "  spark = SparkSession.builder.config(conf = configLocale).getOrCreate()\n",
        "  sc = spark.sparkContext\n",
        "  sc.setLogLevel(\"ERROR\")\n",
        "  \n",
        "  spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\")\n",
        "\n",
        "  # On ajuste l'environnement d'exécution des requêtes à la taille du cluster (4 coeurs)\n",
        "  spark.conf.set(\"spark.sql.shuffle.partitions\",\"4\")    \n",
        "  print(\"session démarrée, son id est \", sc.applicationId)\n",
        "  return spark\n",
        "spark = demarrer_spark()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "session démarrée, son id est  local-1635498300481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "852847c4-bd45-4f9c-ae8b-2c704f389cf9"
        },
        "id": "fj9pUgrmGpLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127de978-2782-429c-ad06-8423eadcd8b0"
      },
      "source": [
        "# on utilise 8 partitions au lieu de 200 par défaut\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
        "print(\"Nombre de partitions utilisées : \", spark.conf.get(\"spark.sql.shuffle.partitions\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de partitions utilisées :  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "41dd8b8e-d697-4efb-a1d2-cd34a08b3e7e"
        },
        "id": "NtjUaom1_2bC"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlMUUoCNQ1fI"
      },
      "source": [
        "data = [[\"young\",\"high\",\"no\",\"fair\",\"no\"],\n",
        "        [\"young\",\"high\",\"no\",\"excellent\",\"no\"],\n",
        "        [\"middle\",\"high\",\"no\",\"fair\",\"yes\"],\n",
        "        [\"senior\",\"low\",\"yes\",\"fair\",\"yes\"],\n",
        "        [\"senior\",\"low\",\"yes\",\"excellent\",\"no\"],\n",
        "        [\"middle\",\"low\",\"yes\",\"excellent\",\"yes\"],\n",
        "        [\"young\",\"medium\",\"no\",\"fair\",\"no\"],\n",
        "         [\"young\",\"low\",\"yes\",\"fair\",\"yes\"],\n",
        "         [\"senior\",\"medium\",\"yes\",\"fair\",\"yes\"],\n",
        "         [\"young\",\"medium\",\"yes\",\"excellent\",\"yes\"],\n",
        "         [\"middle\",\"medium\",\"no\",\"excellent\",\"yes\"],\n",
        "         [\"middle\",\"high\",\"yes\",\"fair\",\"yes\"],\n",
        "         [\"senior\",\"medium\",\"no\",\"excellent\",\"no\"]]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVjToYUYXtv5",
        "outputId": "4a7a6ed0-c893-4040-9591-514485541190"
      },
      "source": [
        "data_df = spark.createDataFrame(spark.sparkContext.parallelize(data),'age String, income String, student String, rating String, label String')\n",
        "data_df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['age', 'income', 'student', 'rating', 'label']"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC5uGrjVa5ni"
      },
      "source": [
        "## Basic fonctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRuCLJW4ehni",
        "outputId": "105da97b-f7a5-42ee-87db-8c5d78d30e34"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+---------+-----+\n",
            "|   age|income|student|   rating|label|\n",
            "+------+------+-------+---------+-----+\n",
            "| young|  high|     no|     fair|   no|\n",
            "| young|  high|     no|excellent|   no|\n",
            "|middle|  high|     no|     fair|  yes|\n",
            "|senior|   low|    yes|     fair|  yes|\n",
            "|senior|   low|    yes|excellent|   no|\n",
            "|middle|   low|    yes|excellent|  yes|\n",
            "| young|medium|     no|     fair|   no|\n",
            "| young|   low|    yes|     fair|  yes|\n",
            "|senior|medium|    yes|     fair|  yes|\n",
            "| young|medium|    yes|excellent|  yes|\n",
            "|middle|medium|     no|excellent|  yes|\n",
            "|middle|  high|    yes|     fair|  yes|\n",
            "|senior|medium|     no|excellent|   no|\n",
            "+------+------+-------+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLszkU01Vyit"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "'''\n",
        "create new columns with index instead of string\n",
        "'''\n",
        "df = data_df\n",
        "for field in df.columns:\n",
        "  indexer = StringIndexer(inputCol=field,outputCol='indexed_'+field)\n",
        "  df = indexer.fit(df).transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQeQiEmsYRpN"
      },
      "source": [
        "from pyspark.ml.feature import IndexToString\n",
        "'''\n",
        "retrieve the string from the index\n",
        "'''\n",
        "age_rev_indexer = IndexToString(inputCol=\"indexed_age\",\n",
        "outputCol='original_age')\n",
        "df_age_retrieved =age_rev_indexer.transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBJZ-Ls1ZC4P"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "'''\n",
        "compute the One hot encoding\n",
        "'''\n",
        "age_onehotenc = OneHotEncoder(inputCol='indexed_age', outputCol='cat_age')\n",
        "age_onehotenc.setDropLast(False)\n",
        "df_age_onehot = age_onehotenc.fit(df).transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq0E8B-1ZOjc"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "'''\n",
        "Concatenate column in one vec\n",
        "'''\n",
        "cols = ['indexed_age','indexed_income']\n",
        "vec_assembler = VectorAssembler(inputCols= cols, outputCol= 'ageIncomeVec')\n",
        "df_age_income_vec = vec_assembler. transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtJwfKdWaCPM",
        "outputId": "2aa22608-d1c0-456c-b62b-109203092a48"
      },
      "source": [
        "from pyspark.ml.feature import VectorIndexer\n",
        "'''\n",
        "reaname values of vector to their index\n",
        "'''\n",
        "\n",
        "vecIndexer = VectorIndexer(inputCol='ageIncomeVec', outputCol='indexed_ageIncomeVec', maxCategories=3)\n",
        "df_age_income_vec_idx = vecIndexer.fit(df_age_income_vec).\\\n",
        "transform(df_age_income_vec) \n",
        "df_age_income_vec_idx.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+---------+-----+-----------+--------------+---------------+--------------+-------------+------------+--------------------+\n",
            "|   age|income|student|   rating|label|indexed_age|indexed_income|indexed_student|indexed_rating|indexed_label|ageIncomeVec|indexed_ageIncomeVec|\n",
            "+------+------+-------+---------+-----+-----------+--------------+---------------+--------------+-------------+------------+--------------------+\n",
            "| young|  high|     no|     fair|   no|        0.0|           1.0|            1.0|           0.0|          1.0|   [0.0,1.0]|           [0.0,1.0]|\n",
            "| young|  high|     no|excellent|   no|        0.0|           1.0|            1.0|           1.0|          1.0|   [0.0,1.0]|           [0.0,1.0]|\n",
            "|middle|  high|     no|     fair|  yes|        1.0|           1.0|            1.0|           0.0|          0.0|   [1.0,1.0]|           [1.0,1.0]|\n",
            "|senior|   low|    yes|     fair|  yes|        2.0|           2.0|            0.0|           0.0|          0.0|   [2.0,2.0]|           [2.0,2.0]|\n",
            "|senior|   low|    yes|excellent|   no|        2.0|           2.0|            0.0|           1.0|          1.0|   [2.0,2.0]|           [2.0,2.0]|\n",
            "|middle|   low|    yes|excellent|  yes|        1.0|           2.0|            0.0|           1.0|          0.0|   [1.0,2.0]|           [1.0,2.0]|\n",
            "| young|medium|     no|     fair|   no|        0.0|           0.0|            1.0|           0.0|          1.0|   (2,[],[])|           (2,[],[])|\n",
            "| young|   low|    yes|     fair|  yes|        0.0|           2.0|            0.0|           0.0|          0.0|   [0.0,2.0]|           [0.0,2.0]|\n",
            "|senior|medium|    yes|     fair|  yes|        2.0|           0.0|            0.0|           0.0|          0.0|   [2.0,0.0]|           [2.0,0.0]|\n",
            "| young|medium|    yes|excellent|  yes|        0.0|           0.0|            0.0|           1.0|          0.0|   (2,[],[])|           (2,[],[])|\n",
            "|middle|medium|     no|excellent|  yes|        1.0|           0.0|            1.0|           1.0|          0.0|   [1.0,0.0]|           [1.0,0.0]|\n",
            "|middle|  high|    yes|     fair|  yes|        1.0|           1.0|            0.0|           0.0|          0.0|   [1.0,1.0]|           [1.0,1.0]|\n",
            "|senior|medium|     no|excellent|   no|        2.0|           0.0|            1.0|           1.0|          1.0|   [2.0,0.0]|           [2.0,0.0]|\n",
            "+------+------+-------+---------+-----+-----------+--------------+---------------+--------------+-------------+------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaRHcih_an-O"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPG3JrQKahPB",
        "outputId": "e3102181-d387-4e62-e43f-92622cecf68d"
      },
      "source": [
        "label = 'label'\n",
        "features_col = data_df.columns\n",
        "features_col.remove(label)\n",
        "prefix = 'indexed_'\n",
        "\n",
        "\n",
        "label_string_indexer = StringIndexer(inputCol=label, outputCol=prefix+label)\n",
        "features_str_col = list(map(lambda c:prefix+c, features_col))\n",
        "features_string_indexer = StringIndexer(inputCols=features_col,outputCols=features_str_col)\n",
        "\n",
        "vec_assembler = VectorAssembler(inputCols= features_string_indexer.getOutputCols(),outputCol= 'vector')\n",
        "\n",
        "vec_indexer = VectorIndexer(inputCol='vector',outputCol='features' ,maxCategories=3)\n",
        "\n",
        "stages = [label_string_indexer,features_string_indexer,vec_assembler,vec_indexer] # stages of the pipeline\n",
        "\n",
        "from pyspark.ml import Pipeline \n",
        "\n",
        "pipeline = Pipeline(stages = stages)\n",
        "train_data = pipeline.fit(data_df).transform(data_df).select(\"features\",\"indexed_label\").withColumnRenamed(\"indexed_label\", \"label\")\n",
        "train_data.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|         features|label|\n",
            "+-----------------+-----+\n",
            "|[0.0,1.0,1.0,0.0]|  1.0|\n",
            "|[0.0,1.0,1.0,1.0]|  1.0|\n",
            "|[1.0,1.0,1.0,0.0]|  0.0|\n",
            "|[2.0,2.0,0.0,0.0]|  0.0|\n",
            "|[2.0,2.0,0.0,1.0]|  1.0|\n",
            "|[1.0,2.0,0.0,1.0]|  0.0|\n",
            "|    (4,[2],[1.0])|  1.0|\n",
            "|    (4,[1],[2.0])|  0.0|\n",
            "|    (4,[0],[2.0])|  0.0|\n",
            "|    (4,[3],[1.0])|  0.0|\n",
            "|[1.0,0.0,1.0,1.0]|  0.0|\n",
            "|[1.0,1.0,0.0,0.0]|  0.0|\n",
            "|[2.0,0.0,1.0,1.0]|  1.0|\n",
            "+-----------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4nLa0cCc6Z2"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRz4Xkemc3gj"
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassificationModel, DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "dtModel = dt.fit(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYT3wGBufibF",
        "outputId": "cc68a341-d2fb-4463-b644-e0301e293163"
      },
      "source": [
        "print(dtModel.toDebugString)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_feb70edf1be3, depth=3, numNodes=9, numClasses=2, numFeatures=4\n",
            "  If (feature 2 in {0.0})\n",
            "   If (feature 0 in {0.0,1.0})\n",
            "    Predict: 0.0\n",
            "   Else (feature 0 not in {0.0,1.0})\n",
            "    If (feature 3 in {0.0})\n",
            "     Predict: 0.0\n",
            "    Else (feature 3 not in {0.0})\n",
            "     Predict: 1.0\n",
            "  Else (feature 2 not in {0.0})\n",
            "   If (feature 0 in {1.0})\n",
            "    Predict: 0.0\n",
            "   Else (feature 0 not in {1.0})\n",
            "    Predict: 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN9BCDPAhyDM"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluatorPR = BinaryClassificationEvaluator()\n",
        "\n",
        "dt_paramGrid = ParamGridBuilder().addGrid(dt.maxBins, [40,42]).addGrid(dt.minInstancesPerNode, [10,100]) .build()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zlqrAeHd7nA",
        "outputId": "1189c7c6-f037-451f-fb21-565bfb66ed51"
      },
      "source": [
        "\n",
        "\n",
        "#create k folds with k=5.\n",
        "cv = CrossValidator(estimator=dt, estimatorParamMaps=dt_paramGrid, evaluator=evaluatorPR, numFolds=5, parallelism=2)\n",
        "\n",
        "\n",
        "cvModel = cv.fit(train_data)\n",
        "\n",
        "bestModel = cvModel.bestModel\n",
        "\n",
        "print(bestModel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_feb70edf1be3, depth=3, numNodes=9, numClasses=2, numFeatures=4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc-3wBbkcPqF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9931a596-3ea9-4d1f-b408-b3558647f934"
        },
        "id": "R5ZJa3iU_2bh"
      },
      "source": [
        "##END"
      ]
    }
  ]
}